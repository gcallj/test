{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "77uYBRWbYufA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762077022214,
     "user_tz": -60,
     "elapsed": 7067,
     "user": {
      "displayName": "Gabriel Juliao",
      "userId": "00918031370409536021"
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip -q install lightgbm pyarrow fastparquet tqdm_joblib imbalanced-learn ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXcYIBB46rF4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762078614741,
     "user_tz": -60,
     "elapsed": 1592502,
     "user": {
      "displayName": "Gabriel Juliao",
      "userId": "00918031370409536021"
     }
    },
    "outputId": "c8676fab-b32a-454c-fa90-5b3c3e00a297"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "Baixando cotações do Yahoo Finance...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: NTCO3.SA\"}}}\n",
      "ERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: XAUEUR\"}}}\n",
      "ERROR:yfinance:\n",
      "11 Failed downloads:\n",
      "ERROR:yfinance:['NTCO3.SA', 'JBSS3.SA', 'XAUEUR', 'NG=F', 'GOLL4.SA', 'XAGEUR', 'VVAR3.SA', 'CIEL3.SA', 'CESP6.SA', 'CCRO3.SA', 'LAME4.SA']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Período: 2005-01-03 → 2025-11-02\n",
      "Tickers com dados: 147\n",
      "Gerando features e targets por ticker...\n",
      "Gerando features e targets por ticker...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 147/147 [05:34<00:00,  2.28s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tickers processados: 147 | Motivos de skip: {}\n",
      "[Feature reduction] columns: 24696 → 14435 (58.5% kept)\n",
      "Saved FULL:     drive/My Drive/Colab Notebooks/stock/expanded_stock.parquet  shape=(6597, 25284)\n",
      "Saved REDUCED:  drive/My Drive/Colab Notebooks/stock/expanded_stock_reduced.parquet  shape=(6597, 15023)\n",
      "X full: (6597, 24696)\n",
      "y: (6597, 588)\n",
      "X_reduced: (6597, 14435)\n",
      "Min/median/max kept per ticker: 15 103.0 122\n",
      "Processed tickers: 147\n",
      "Missing tickers: 0\n"
     ]
    }
   ],
   "source": [
    "!pip -q install lightgbm pyarrow fastparquet tqdm imbalanced-learn ta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ta  # technical analysis\n",
    "from google.colab import drive\n",
    "\n",
    "# ============================\n",
    "# Configurações gerais\n",
    "# ============================\n",
    "START_DATE = \"2005-01-01\"\n",
    "\n",
    "# Permite preencher lacunas de FEATURES com bfill (útil p/ calendários diferentes).\n",
    "# Isto pode introduzir leakage \"quando inevitável\".\n",
    "ALLOW_BFILL_EXOGENOUS = True\n",
    "\n",
    "# Defasagem das FEATURES (1 evita leakage trivial; 0 permite mais vazamento).\n",
    "SHIFT_FEATURES = 0\n",
    "\n",
    "# Médias móveis a usar (manteremos TODOS cruzamentos slow > fast)\n",
    "AVERAGES = [1, 2, 5, 10, 15, 20, 25, 50, 100]\n",
    "\n",
    "# Horizonte para cálculo de alvos\n",
    "HORIZON = 90\n",
    "UP_THR = 0.30   # +30%\n",
    "DD_THR = -0.10  # -10%\n",
    "\n",
    "\n",
    "SAVE_PARQUET = True\n",
    "SAVE_CSV_FALLBACK = False\n",
    "OUTPUT_PATH = \"drive/My Drive/Colab Notebooks/stock/expanded_stock.parquet\"\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ============================\n",
    "# Listas de tickers\n",
    "# ============================\n",
    "ibovespa_tickers = [\n",
    "    'ABEV3.SA', 'B3SA3.SA', 'BBAS3.SA', 'BBDC4.SA', 'BBSE3.SA', 'BRFS3.SA', 'BRKM5.SA', 'CCRO3.SA',\n",
    "    'CIEL3.SA', 'CMIG4.SA', 'CSAN3.SA', 'CSNA3.SA', 'CVCB3.SA', 'CYRE3.SA', 'ECOR3.SA', 'EGIE3.SA', 'ELET3.SA', 'EMBR3.SA',\n",
    "    'ENGI11.SA', 'EQTL3.SA', 'EVEN3.SA', 'FIBR3.SA', 'GGBR4.SA', 'HAPV3.SA', 'ITUB4.SA', 'JBSS3.SA',\n",
    "    'JHSF3.SA', 'LAME4.SA', 'LOGG3.SA', 'LREN3.SA', 'MULT3.SA', 'NATU3.SA', 'MRFG3.SA', 'MOVI3.SA',\n",
    "    'MYPK3.SA', 'MDIA3.SA', 'IRBR3.SA', 'NTCO3.SA', 'PETR3.SA', 'PETR4.SA', 'PRIO3.SA', 'RADL3.SA',\n",
    "    'RAIL3.SA', 'RENT3.SA', 'RAIZ4.SA', 'SBSP3.SA', 'SANB3.SA', 'SAPR3.SA', 'SUZB3.SA', 'TCSA3.SA',\n",
    "    'VIVA3.SA', 'AZUL4.SA', 'GOLL4.SA', 'WEGE3.SA','BBDC3.SA', 'VVAR3.SA', 'BEEF3.SA', 'CESP6.SA',\n",
    "    'USIM5.SA', 'VALE3.SA', 'POMO4.SA', 'LEVE3.SA', 'TUPY3.SA', 'RAPT4.SA', 'ROMI3.SA'\n",
    "]\n",
    "\n",
    "fii_tickers = [\n",
    "    'MXRF11.SA','HGLG11.SA','KNRI11.SA','VISC11.SA','XPLG11.SA','VILG11.SA','BTLG11.SA',\n",
    "    'BRCO11.SA','GGRC11.SA','LVBI11.SA','XPML11.SA','HSML11.SA',\n",
    "    'BRCR11.SA','HGRE11.SA','PVBI11.SA','RCRB11.SA','VINO11.SA',\n",
    "    'ALZR11.SA','TRXF11.SA','RBVA11.SA','RBRP11.SA',\n",
    "    'KNCR11.SA','KNHY11.SA','KNSC11.SA','CPTS11.SA','HCTR11.SA','IRDM11.SA','URPR11.SA',\n",
    "    'OUJP11.SA','VRTA11.SA','HGCR11.SA','DEVA11.SA','RBRR11.SA',\n",
    "    'HFOF11.SA','KFOF11.SA','XPSF11.SA','RBRF11.SA','VGHF11.SA',\n",
    "]\n",
    "\n",
    "global_indices = [\n",
    "    '^GSPC', '^DJI', '^IXIC', '^FTSE', '^FCHI', '^GDAXI', '^N225', '^HSI', '^AXJO', '^BSESN', '^SSE', '^JKSE', '^BVSP'\n",
    "]\n",
    "\n",
    "currency_commodity_tickers = [\n",
    "    'UUP','FXE','FXY','GLD','USO',\n",
    "    'EURUSD=X','GBPUSD=X','CNYUSD=X','AUDUSD=X','CHFUSD=X','BRLUSD=X','MXNUSD=X',\n",
    "    'BTC-USD','ETH-USD','DOGE-USD','LTC-USD','SOL-USD',\n",
    "    'CL=F','GC=F','NG=F','HG=F','ZC=F','HE=F','ZW=F','S=F','BZ=F',\n",
    "    'XAUEUR','XAGEUR','COPX','MGC=F','HO=F'\n",
    "]\n",
    "\n",
    "ALL_TICKERS = ibovespa_tickers + fii_tickers + global_indices + currency_commodity_tickers\n",
    "\n",
    "# ============================\n",
    "# Utilidades de preenchimento/casting\n",
    "# ============================\n",
    "def fill_100pct(df: pd.DataFrame, allow_bfill=True) -> pd.DataFrame:\n",
    "    \"\"\"Garante 100% preenchido: Inf->NaN, ffill, bfill opcional, e NaN restantes->0.\"\"\"\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.ffill()\n",
    "    if allow_bfill:\n",
    "        df = df.bfill()\n",
    "    # Se alguma coluna ficou toda NaN (pode ocorrer em padrões), zera\n",
    "    all_nan_cols = df.columns[df.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        df[all_nan_cols] = 0.0\n",
    "    # NaN remanescentes -> 0\n",
    "    df = df.fillna(0.0)\n",
    "    return df\n",
    "\n",
    "def cast_int8_multi(df: pd.DataFrame, prefixes=(), suffixes=()):\n",
    "    if not isinstance(df.columns, pd.MultiIndex):\n",
    "        return df\n",
    "\n",
    "    level0 = pd.Index(df.columns.get_level_values(0).astype(str))\n",
    "    mask = np.zeros(len(level0), dtype=bool)\n",
    "\n",
    "    if prefixes:\n",
    "        starts = level0.str.startswith(prefixes)          # array-like\n",
    "        mask = np.logical_or(mask, np.asarray(starts, dtype=bool))\n",
    "\n",
    "    if suffixes:\n",
    "        ends = level0.str.endswith(suffixes)              # array-like\n",
    "        mask = np.logical_or(mask, np.asarray(ends, dtype=bool))\n",
    "\n",
    "    cols = df.columns[mask]\n",
    "    if len(cols):\n",
    "        df = df.copy()\n",
    "        df.loc[:, cols] = df.loc[:, cols].astype('int8', copy=False)\n",
    "    return df\n",
    "\n",
    "# ============================\n",
    "# Download dos dados\n",
    "# ============================\n",
    "print(\"Baixando cotações do Yahoo Finance...\")\n",
    "data = yf.download(\n",
    "    ALL_TICKERS,\n",
    "    start=START_DATE,\n",
    "    group_by='column',\n",
    "    auto_adjust=True,\n",
    "    progress=False,\n",
    "    threads=True\n",
    ")\n",
    "\n",
    "# Somente colunas OHLCV relevantes e limpeza de levels\n",
    "allowed_columns = ['Open','High','Low','Close','Adj Close','Volume']\n",
    "data = data.loc[:, data.columns.get_level_values(0).isin(allowed_columns)].copy()\n",
    "data.columns = data.columns.remove_unused_levels()\n",
    "\n",
    "# Forward-fill para alinhar calendários; (bfill só nas FEATURES mais adiante)\n",
    "data = data.ffill()\n",
    "\n",
    "# Tickers efetivamente presentes\n",
    "tickers = np.unique(data.columns.get_level_values(1))\n",
    "print(f\"Período: {data.index.min().date()} → {data.index.max().date()}\")\n",
    "print(f\"Tickers com dados: {len(tickers)}\")\n",
    "\n",
    "# ============================\n",
    "# Funções de padrões (com prefixos)\n",
    "# ============================\n",
    "def detect_head_shoulder(df, window=3, prefix=\"hs_\"):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[prefix+'high_roll_max'] = df['High'].rolling(window).max()\n",
    "    out[prefix+'low_roll_min']  = df['Low'].rolling(window).min()\n",
    "    mask_hs  = ((out[prefix+'high_roll_max'] > df['High'].shift(1)) &\n",
    "                (out[prefix+'high_roll_max'] > df['High'].shift(-1)) &\n",
    "                (df['High'] < df['High'].shift(1)) &\n",
    "                (df['High'] < df['High'].shift(-1)))\n",
    "    mask_inv = ((out[prefix+'low_roll_min'] < df['Low'].shift(1)) &\n",
    "                (out[prefix+'low_roll_min'] < df['Low'].shift(-1)) &\n",
    "                (df['Low'] > df['Low'].shift(1)) &\n",
    "                (df['Low'] > df['Low'].shift(-1)))\n",
    "    out[prefix+'pattern'] = 0\n",
    "    out.loc[mask_hs,  prefix+'pattern'] = 1\n",
    "    out.loc[mask_inv, prefix+'pattern'] = -1\n",
    "    return out\n",
    "\n",
    "def detect_multiple_tops_bottoms(df, window=3, prefix=\"mtb_\"):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[prefix+'high_roll_max']  = df['High'].rolling(window).max()\n",
    "    out[prefix+'low_roll_min']   = df['Low'].rolling(window).min()\n",
    "    out[prefix+'close_roll_max'] = df['Close'].rolling(window).max()\n",
    "    out[prefix+'close_roll_min'] = df['Close'].rolling(window).min()\n",
    "    mask_top    = (out[prefix+'high_roll_max'] >= df['High'].shift(1)) & (out[prefix+'close_roll_max'] < df['Close'].shift(1))\n",
    "    mask_bottom = (out[prefix+'low_roll_min']  <= df['Low'].shift(1))  & (out[prefix+'close_roll_min']  > df['Close'].shift(1))\n",
    "    out[prefix+'pattern'] = 0\n",
    "    out.loc[mask_top,    prefix+'pattern'] = 1\n",
    "    out.loc[mask_bottom, prefix+'pattern'] = -1\n",
    "    return out\n",
    "\n",
    "def calculate_support_resistance(df, window=3, prefix=\"sr_\"):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    mean_high = df['High'].rolling(window).mean()\n",
    "    std_high  = df['High'].rolling(window).std()\n",
    "    mean_low  = df['Low'].rolling(window).mean()\n",
    "    std_low   = df['Low'].rolling(window).std()\n",
    "    out[prefix+'support']     = mean_low - 2*std_low\n",
    "    out[prefix+'resistance']  = mean_high + 2*std_high\n",
    "    out[prefix+'diff_support']    = df['Close'] - out[prefix+'support']\n",
    "    out[prefix+'diff_resistance'] = out[prefix+'resistance'] - df['Close']\n",
    "    return out\n",
    "\n",
    "def detect_triangle_pattern(df, window=3, prefix=\"tri_\"):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[prefix+'high_roll_max'] = df['High'].rolling(window).max()\n",
    "    out[prefix+'low_roll_min']  = df['Low'].rolling(window).min()\n",
    "    mask_asc  = (out[prefix+'high_roll_max'] >= df['High'].shift(1)) & (out[prefix+'low_roll_min'] <= df['Low'].shift(1)) & (df['Close'] > df['Close'].shift(1))\n",
    "    mask_desc = (out[prefix+'high_roll_max'] <= df['High'].shift(1)) & (out[prefix+'low_roll_min'] >= df['Low'].shift(1)) & (df['Close'] < df['Close'].shift(1))\n",
    "    out[prefix+'pattern'] = 0\n",
    "    out.loc[mask_asc,  prefix+'pattern'] = 1\n",
    "    out.loc[mask_desc, prefix+'pattern'] = -1\n",
    "    return out\n",
    "\n",
    "def detect_wedge(df, window=3, prefix=\"wed_\"):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[prefix+'high_roll_max'] = df['High'].rolling(window).max()\n",
    "    out[prefix+'low_roll_min']  = df['Low'].rolling(window).min()\n",
    "    trend_high = df['High'].rolling(window).apply(lambda x: 1 if (x[-1]-x[0])>0 else (-1 if (x[-1]-x[0])<0 else 0), raw=True)\n",
    "    trend_low  = df['Low'].rolling(window).apply(lambda x: 1 if (x[-1]-x[0])>0 else (-1 if (x[-1]-x[0])<0 else 0), raw=True)\n",
    "    mask_up   = (out[prefix+'high_roll_max'] >= df['High'].shift(1)) & (out[prefix+'low_roll_min'] <= df['Low'].shift(1)) & (trend_high == 1) & (trend_low == 1)\n",
    "    mask_down = (out[prefix+'high_roll_max'] <= df['High'].shift(1)) & (out[prefix+'low_roll_min'] >= df['Low'].shift(1)) & (trend_high == -1) & (trend_low == -1)\n",
    "    out[prefix+'pattern'] = 0\n",
    "    out.loc[mask_up,   prefix+'pattern'] = 1\n",
    "    out.loc[mask_down, prefix+'pattern'] = -1\n",
    "    return out\n",
    "\n",
    "def detect_channel(df, window=3, prefix=\"chan_\", channel_range=0.1):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[prefix+'high_roll_max'] = df['High'].rolling(window).max()\n",
    "    out[prefix+'low_roll_min']  = df['Low'].rolling(window).min()\n",
    "    trend_high = df['High'].rolling(window).apply(lambda x: 1 if (x[-1]-x[0])>0 else (-1 if (x[-1]-x[0])<0 else 0), raw=True)\n",
    "    trend_low  = df['Low'].rolling(window).apply(lambda x: 1 if (x[-1]-x[0])>0 else (-1 if (x[-1]-x[0])<0 else 0), raw=True)\n",
    "    width = out[prefix+'high_roll_max'] - out[prefix+'low_roll_min']\n",
    "    mid   = (out[prefix+'high_roll_max'] + out[prefix+'low_roll_min'])/2\n",
    "    mask_up   = (out[prefix+'high_roll_max'] >= df['High'].shift(1)) & (out[prefix+'low_roll_min'] <= df['Low'].shift(1)) & (width <= channel_range*mid) & (trend_high==1) & (trend_low==1)\n",
    "    mask_down = (out[prefix+'high_roll_max'] <= df['High'].shift(1)) & (out[prefix+'low_roll_min'] >= df['Low'].shift(1)) & (width <= channel_range*mid) & (trend_high==-1) & (trend_low==-1)\n",
    "    out[prefix+'pattern'] = 0\n",
    "    out.loc[mask_up,   prefix+'pattern'] = 1\n",
    "    out.loc[mask_down, prefix+'pattern'] = -1\n",
    "    return out\n",
    "\n",
    "def detect_double_top_bottom(df, window=3, threshold=0.05, prefix=\"dbl_\"):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[prefix+'high_roll_max'] = df['High'].rolling(window).max()\n",
    "    out[prefix+'low_roll_min']  = df['Low'].rolling(window).min()\n",
    "    mask_top = (out[prefix+'high_roll_max'] >= df['High'].shift(1)) & (out[prefix+'high_roll_max'] >= df['High'].shift(-1)) & \\\n",
    "               (df['High'] < df['High'].shift(1)) & (df['High'] < df['High'].shift(-1)) & \\\n",
    "               ((df['High'].shift(1)-df['Low'].shift(1)) <= threshold*(df['High'].shift(1)+df['Low'].shift(1))/2) & \\\n",
    "               ((df['High'].shift(-1)-df['Low'].shift(-1)) <= threshold*(df['High'].shift(-1)+df['Low'].shift(-1))/2)\n",
    "    mask_bottom = (out[prefix+'low_roll_min'] <= df['Low'].shift(1)) & (out[prefix+'low_roll_min'] <= df['Low'].shift(-1)) & \\\n",
    "                  (df['Low'] > df['Low'].shift(1)) & (df['Low'] > df['Low'].shift(-1)) & \\\n",
    "                  ((df['High'].shift(1)-df['Low'].shift(1)) <= threshold*(df['High'].shift(1)+df['Low'].shift(1))/2) & \\\n",
    "                  ((df['High'].shift(-1)-df['Low'].shift(-1)) <= threshold*(df['High'].shift(-1)+df['Low'].shift(-1))/2)\n",
    "    out[prefix+'pattern'] = 0\n",
    "    out.loc[mask_top,    prefix+'pattern'] = 1\n",
    "    out.loc[mask_bottom, prefix+'pattern'] = -1\n",
    "    return out\n",
    "\n",
    "def detect_trendline(df, window=2, prefix=\"trend_\"):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    slope = np.zeros(len(df), dtype='float64')\n",
    "    intercept = np.zeros(len(df), dtype='float64')\n",
    "    idx = np.arange(len(df))\n",
    "    close = df['Close'].values\n",
    "    for i in range(window, len(df)):\n",
    "        x = idx[i-window:i].astype(float)\n",
    "        y = close[i-window:i]\n",
    "        A = np.vstack([x, np.ones_like(x)]).T\n",
    "        m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "        slope[i] = m\n",
    "        intercept[i] = c\n",
    "    out[prefix+'slope'] = slope\n",
    "    out[prefix+'intercept'] = intercept\n",
    "    x_now = idx.astype(float)\n",
    "    y_line = slope * x_now + intercept\n",
    "    # Estes dois podem ficar inteiros NaN; trataremos depois com fill_100pct\n",
    "    out[prefix+'support2'] = np.where(slope>0, y_line, np.nan)\n",
    "    out[prefix+'resistance2'] = np.where(slope<0, y_line, np.nan)\n",
    "    out[prefix+'diff_support2'] = df['Close'] - out[prefix+'support2']\n",
    "    out[prefix+'diff_resistance2'] = out[prefix+'resistance2'] - df['Close']\n",
    "    return out\n",
    "# ============================\n",
    "# New PATH-AWARE targets (split)\n",
    "# ============================\n",
    "def make_targets_up_down(df, horizon=30, up_thr=0.20, dd_thr=-0.05,\n",
    "                         name_up='target_up20', name_dd='target_dd5',\n",
    "                         keep_order=False, name_order='target_up_before_dd'):\n",
    "    \"\"\"\n",
    "    For each day t:\n",
    "      - target_up20[t] = 1 if any High in (t+1 ... t+horizon) >= Close[t]*(1+up_thr), else 0\n",
    "      - target_dd5[t]  = 1 if any Low  in (t+1 ... t+horizon) <= Close[t]*(1+dd_thr), else 0\n",
    "    If keep_order=True:\n",
    "      - target_up_before_dd[t] = 1 if the first hit is the UP threshold, 0 if first is DOWN, -1 if none hit\n",
    "    \"\"\"\n",
    "    close = df['Close'].values\n",
    "    high  = df['High'].values\n",
    "    low   = df['Low'].values\n",
    "    n = len(df)\n",
    "\n",
    "    up_hit = np.zeros(n, dtype='int8')\n",
    "    dd_hit = np.zeros(n, dtype='int8')\n",
    "    order  = np.full(n, -1, dtype='int8')  # -1 => none\n",
    "\n",
    "    for t in range(n):\n",
    "        end = min(n, t + horizon + 1)\n",
    "        if end - t <= 1:\n",
    "            continue\n",
    "\n",
    "        entry = close[t]\n",
    "        up_th = entry * (1.0 + up_thr)\n",
    "        dd_th = entry * (1.0 + dd_thr)\n",
    "\n",
    "        hseg = high[t+1:end]\n",
    "        lseg = low[t+1:end]\n",
    "\n",
    "        up_idx = np.where(hseg >= up_th)[0]\n",
    "        dd_idx = np.where(lseg <= dd_th)[0]\n",
    "\n",
    "        hit_up = up_idx[0] if len(up_idx) else None\n",
    "        hit_dd = dd_idx[0] if len(dd_idx) else None\n",
    "\n",
    "        if hit_up is not None:\n",
    "            up_hit[t] = 1\n",
    "        if hit_dd is not None:\n",
    "            dd_hit[t] = 1\n",
    "\n",
    "        if keep_order:\n",
    "            if hit_up is None and hit_dd is None:\n",
    "                order[t] = -1\n",
    "            elif hit_up is None:\n",
    "                order[t] = 0\n",
    "            elif hit_dd is None:\n",
    "                order[t] = 1\n",
    "            else:\n",
    "                order[t] = 1 if hit_up < hit_dd else 0\n",
    "\n",
    "    s_up   = pd.Series(up_hit, index=df.index, name=name_up)\n",
    "    s_down = pd.Series(dd_hit, index=df.index, name=name_dd)\n",
    "    if keep_order:\n",
    "        s_ord = pd.Series(order, index=df.index, name=name_order)\n",
    "        return s_up, s_down, s_ord\n",
    "    else:\n",
    "        return s_up, s_down\n",
    "\n",
    "# ============================\n",
    "# FEATURES (ta + cruzamentos + padrões)\n",
    "# ============================\n",
    "def indicators_for_ticker(ohlcv: pd.DataFrame, shift_features: int = 1) -> pd.DataFrame:\n",
    "    close_col = 'Adj Close' if 'Adj Close' in ohlcv.columns else 'Close'\n",
    "\n",
    "    feats = ta.add_all_ta_features(\n",
    "        ohlcv.copy(),\n",
    "        open=\"Open\", high=\"High\", low=\"Low\", close=close_col, volume=\"Volume\",\n",
    "        fillna=True\n",
    "    )\n",
    "\n",
    "    # SMAs sobre o mesmo close_col\n",
    "    for avg in AVERAGES:\n",
    "        feats[f'SMA_{avg}'] = ohlcv[close_col].rolling(avg).mean()\n",
    "\n",
    "    # cruzamentos\n",
    "    for fast in AVERAGES:\n",
    "        for slow in AVERAGES:\n",
    "            if slow > fast:\n",
    "                fcol = f'SMA_{fast}'\n",
    "                scol = f'SMA_{slow}'\n",
    "                prev_f = feats[fcol].shift(1)\n",
    "                prev_s = feats[scol].shift(1)\n",
    "                crossname = f\"cross_{fast}_{slow}\"\n",
    "                cross = pd.Series(0, index=feats.index, dtype='int8')\n",
    "                cross[(feats[fcol] < feats[scol]) & (prev_f >= prev_s)] = -1\n",
    "                cross[(feats[fcol] > feats[scol]) & (prev_f <= prev_s)] = 1\n",
    "                feats[crossname] = cross\n",
    "\n",
    "    feats['pct_change'] = ohlcv[close_col].pct_change()\n",
    "\n",
    "    if shift_features > 0:\n",
    "        feats = feats.shift(shift_features)\n",
    "\n",
    "    int_cols = [c for c in feats.columns if str(c).startswith(\"cross_\")]\n",
    "    feats[int_cols] = feats[int_cols].astype('int8')\n",
    "    float_cols = [c for c in feats.columns if c not in int_cols]\n",
    "    feats[float_cols] = feats[float_cols].astype('float32')\n",
    "    return feats\n",
    "\n",
    "\n",
    "def patterns_for_ticker(ohlcv: pd.DataFrame, shift_features: int = 1) -> pd.DataFrame:\n",
    "    parts = [\n",
    "        detect_head_shoulder(ohlcv),\n",
    "        detect_multiple_tops_bottoms(ohlcv),\n",
    "        calculate_support_resistance(ohlcv),\n",
    "        detect_triangle_pattern(ohlcv),\n",
    "        detect_wedge(ohlcv),\n",
    "        detect_channel(ohlcv),\n",
    "        detect_double_top_bottom(ohlcv),\n",
    "        detect_trendline(ohlcv),\n",
    "    ]\n",
    "    P = pd.concat(parts, axis=1)\n",
    "    if shift_features > 0:\n",
    "        P = P.shift(shift_features)\n",
    "    patt_cols = [c for c in P.columns if c.endswith('pattern')]\n",
    "    P[patt_cols] = P[patt_cols].astype('int8')\n",
    "    other_cols = [c for c in P.columns if c not in patt_cols]\n",
    "    P[other_cols] = P[other_cols].astype('float32')\n",
    "    return P\n",
    "\n",
    "# ============================\n",
    "# TARGETS\n",
    "# ============================\n",
    "def make_target_path_aware(df, horizon=30, up=0.15, dd=-0.05, name='target_path'):\n",
    "    close = df['Close'].values\n",
    "    high  = df['High'].values\n",
    "    low   = df['Low'].values\n",
    "    n = len(df)\n",
    "    tgt = np.zeros(n, dtype='int8')\n",
    "    for t in range(n):\n",
    "        end = min(n, t + horizon + 1)\n",
    "        if end - t <= 1:\n",
    "            tgt[t] = 0\n",
    "            continue\n",
    "        entry = close[t]\n",
    "        up_th = entry * (1.0 + up)\n",
    "        dd_th = entry * (1.0 + dd)\n",
    "        hseg = high[t+1:end]\n",
    "        lseg = low[t+1:end]\n",
    "        hit_up_idx = np.where(hseg >= up_th)[0]\n",
    "        hit_dd_idx = np.where(lseg <= dd_th)[0]\n",
    "        hit_up = hit_up_idx[0] if len(hit_up_idx) else None\n",
    "        hit_dd = hit_dd_idx[0] if len(hit_dd_idx) else None\n",
    "        tgt[t] = 1 if (hit_up is not None and (hit_dd is None or hit_up < hit_dd)) else 0\n",
    "    return pd.Series(tgt, index=df.index, name=name)\n",
    "\n",
    "def make_best_entry_sale(df, horizon=30):\n",
    "    low = df['Low'].values\n",
    "    high = df['High'].values\n",
    "    n = len(df)\n",
    "    best_entry = np.empty(n, dtype='float32')\n",
    "    best_sale  = np.empty(n, dtype='float32')\n",
    "    for t in range(n):\n",
    "        end = min(n, t + horizon + 1)\n",
    "        window_low = low[t:end]\n",
    "        window_high = high[t:end]\n",
    "        best_entry[t] = float(np.nanmin(window_low))\n",
    "        best_sale[t]  = float(np.nanmax(window_high))\n",
    "    s_entry = pd.Series(best_entry, index=df.index, name='target_best_entry')\n",
    "    s_sale  = pd.Series(best_sale,  index=df.index, name='target_best_sale')\n",
    "    return s_entry, s_sale\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# ============================\n",
    "# Feature reduction helpers\n",
    "# ============================\n",
    "def _is_discrete_series(colname: str) -> bool:\n",
    "    # treat crosses and pattern flags as discrete\n",
    "    return colname.startswith(\"cross_\") or colname.endswith(\"pattern\")\n",
    "\n",
    "def _drop_near_constant(df_tk: pd.DataFrame, var_thr: float = 1e-12):\n",
    "    variances = df_tk.var(axis=0).astype(float)\n",
    "    keep = variances > var_thr\n",
    "    return df_tk.loc[:, keep], keep.index[keep].tolist()\n",
    "\n",
    "def _mi_rank_per_ticker(X_tk: pd.DataFrame, y_up: pd.Series, y_dd: pd.Series,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    Compute MI vs both targets; take max(MI_up, MI_dd) per feature.\n",
    "    \"\"\"\n",
    "    # mask discrete\n",
    "    cols = X_tk.columns.tolist()\n",
    "    discrete_mask = np.array([_is_discrete_series(c) for c in cols], dtype=bool)\n",
    "\n",
    "    # y must be 1D arrays\n",
    "    y_up_arr = y_up.astype('int8').values\n",
    "    y_dd_arr = y_dd.astype('int8').values\n",
    "\n",
    "    rs = check_random_state(random_state)\n",
    "    # MI can fail on constant features; ensure X already filtered\n",
    "    mi_up = mutual_info_classif(X_tk.values, y_up_arr,\n",
    "                                discrete_features=discrete_mask,\n",
    "                                random_state=rs)\n",
    "    mi_dd = mutual_info_classif(X_tk.values, y_dd_arr,\n",
    "                                discrete_features=discrete_mask,\n",
    "                                random_state=rs)\n",
    "    mi = np.maximum(mi_up, mi_dd)\n",
    "    mi_s = pd.Series(mi, index=cols).sort_values(ascending=False)\n",
    "    return mi_s\n",
    "\n",
    "def _greedy_cor_filter(X_tk: pd.DataFrame, ranking: pd.Series,\n",
    "                       corr_thr: float = 0.995):\n",
    "    \"\"\"\n",
    "    Keep features in order of 'ranking' (desc), discard any that\n",
    "    correlate (|r| >= corr_thr) with a feature already kept.\n",
    "    \"\"\"\n",
    "    if X_tk.shape[1] <= 1:\n",
    "        return X_tk.columns.tolist()\n",
    "\n",
    "    ordered = [c for c in ranking.index if c in X_tk.columns]\n",
    "    keep = []\n",
    "    # precompute correlation in chunks to save time\n",
    "    # Pearson on normalized data\n",
    "    Z = (X_tk - X_tk.mean()) / (X_tk.std(ddof=0) + 1e-12)\n",
    "\n",
    "    for c in ordered:\n",
    "        if not keep:\n",
    "            keep.append(c)\n",
    "            continue\n",
    "        # correlate c with kept\n",
    "        r = Z[keep].T.dot(Z[c]) / (len(Z) - 1)\n",
    "        max_abs_r = np.abs(r.values).max()\n",
    "        if not np.isfinite(max_abs_r) or max_abs_r < corr_thr:\n",
    "            keep.append(c)\n",
    "    return keep\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def reduce_features_automatic(\n",
    "    X: pd.DataFrame,\n",
    "    y_up: pd.DataFrame,\n",
    "    y_dd: pd.DataFrame,\n",
    "    top_fraction: float = 0.35,\n",
    "    min_keep: int = 48,\n",
    "    var_thr: float | None = None,     # <<< added back\n",
    "    corr_thr: float = 0.995,\n",
    "    always_keep_prefixes=(\"pct_change\",\"SMA_20\",\"SMA_50\",\"SMA_100\",\"tri_\",\"sr_\"),\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust per-ticker reduction:\n",
    "      - drop constants (nunique > 1) and optionally low-variance (<= var_thr)\n",
    "      - MI vs both targets; fallback to variance if MI is flat/constant\n",
    "      - greedy correlation de-dup; pad back to min_keep\n",
    "    \"\"\"\n",
    "    if not isinstance(X.columns, pd.MultiIndex):\n",
    "        raise ValueError(\"X must use MultiIndex columns=(feature, ticker).\")\n",
    "\n",
    "    tickers = np.unique(X.columns.get_level_values(1))\n",
    "    kept_cols = []\n",
    "    before_cnt = X.shape[1]\n",
    "    rs = check_random_state(42)\n",
    "\n",
    "    up_tk_set = set(y_up.columns.get_level_values(1))\n",
    "    dd_tk_set = set(y_dd.columns.get_level_values(1))\n",
    "\n",
    "    for tk in tickers:\n",
    "        X_tk = X.xs(tk, level=1, axis=1).copy()\n",
    "\n",
    "        # numeric only + ensure no NaN/Inf\n",
    "        X_tk = X_tk.apply(pd.to_numeric, errors='coerce').replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "        # drop constants\n",
    "        nunique = X_tk.nunique(dropna=False)\n",
    "        X_tk_nc = X_tk.loc[:, nunique > 1]\n",
    "\n",
    "        # optional variance threshold\n",
    "        if var_thr is not None and X_tk_nc.shape[1] > 0:\n",
    "            variances = X_tk_nc.var().astype(float)\n",
    "            X_tk_nc = X_tk_nc.loc[:, variances > var_thr]\n",
    "\n",
    "        if X_tk_nc.shape[1] == 0:\n",
    "            base = [c for c in X_tk.columns if any(str(c).startswith(p) for p in always_keep_prefixes)]\n",
    "            base = base[:min_keep] if base else X_tk.columns[:min_keep].tolist()\n",
    "            kept_cols.extend([(c, tk) for c in base])\n",
    "            continue\n",
    "\n",
    "        # if labels missing for ticker, keep top variance\n",
    "        if (tk not in up_tk_set) or (tk not in dd_tk_set):\n",
    "            var_rank = X_tk_nc.var().sort_values(ascending=False)\n",
    "            base = var_rank.index[:min(min_keep, len(var_rank))].tolist()\n",
    "            kept_cols.extend([(c, tk) for c in base])\n",
    "            continue\n",
    "\n",
    "        y_up_tk = y_up.xs(tk, level=1, axis=1).iloc[:,0].astype('int8')\n",
    "        y_dd_tk = y_dd.xs(tk, level=1, axis=1).iloc[:,0].astype('int8')\n",
    "\n",
    "        cols = X_tk_nc.columns.tolist()\n",
    "        discrete_mask = np.array([str(c).startswith('cross_') or str(c).endswith('pattern') for c in cols], dtype=bool)\n",
    "\n",
    "        def safe_mi(Xarr, yarr):\n",
    "            if np.unique(yarr).size < 2:\n",
    "                return np.zeros(Xarr.shape[1], dtype=float)\n",
    "            try:\n",
    "                return mutual_info_classif(Xarr, yarr, discrete_features=discrete_mask, random_state=rs)\n",
    "            except Exception:\n",
    "                return np.zeros(Xarr.shape[1], dtype=float)\n",
    "\n",
    "        mi_up = safe_mi(X_tk_nc.values, y_up_tk.values)\n",
    "        mi_dd = safe_mi(X_tk_nc.values, y_dd_tk.values)\n",
    "        mi = np.maximum(mi_up, mi_dd)\n",
    "\n",
    "        mi_rank = pd.Series(mi, index=cols).sort_values(ascending=False)\n",
    "        k_top = min(len(mi_rank), max(min_keep, int(np.ceil(len(mi_rank) * top_fraction))))\n",
    "        top_feats = mi_rank.index[:k_top].tolist()\n",
    "\n",
    "        # ensure interpretable anchors\n",
    "        for pref in always_keep_prefixes:\n",
    "            top_feats.extend([c for c in X_tk_nc.columns if str(c).startswith(pref)])\n",
    "        # de-dup order\n",
    "        seen = set()\n",
    "        top_feats = [c for c in top_feats if not (c in seen or seen.add(c))]\n",
    "\n",
    "        # fallback if MI flat\n",
    "        if len(top_feats) == 0 or (mi_rank.iloc[0] == 0 and mi_rank.sum() == 0):\n",
    "            var_rank = X_tk_nc.var().sort_values(ascending=False)\n",
    "            top_feats = var_rank.index[:min(min_keep, len(var_rank))].tolist()\n",
    "\n",
    "        # greedy correlation de-dup\n",
    "        X_top = X_tk_nc[top_feats]\n",
    "        Z = (X_top - X_top.mean()) / (X_top.std(ddof=0) + 1e-12)\n",
    "        keep = []\n",
    "        for c in X_top.columns:\n",
    "            if not keep:\n",
    "                keep.append(c); continue\n",
    "            r = Z[keep].T.dot(Z[c]) / max(1, (len(Z) - 1))\n",
    "            max_abs_r = np.abs(r.values).max() if hasattr(r, \"values\") else float(np.abs(r).max())\n",
    "            if not np.isfinite(max_abs_r) or max_abs_r < corr_thr:\n",
    "                keep.append(c)\n",
    "\n",
    "        # pad to min_keep\n",
    "        if len(keep) < min_keep:\n",
    "            for c in top_feats:\n",
    "                if c not in keep:\n",
    "                    keep.append(c)\n",
    "                    if len(keep) >= min_keep:\n",
    "                        break\n",
    "\n",
    "        kept_cols.extend([(c, tk) for c in keep])\n",
    "\n",
    "    if len(kept_cols) == 0:\n",
    "        for tk in tickers:\n",
    "            cols_tk = X.xs(tk, level=1, axis=1).columns[:min_keep].tolist()\n",
    "            kept_cols.extend([(c, tk) for c in cols_tk])\n",
    "\n",
    "    kept_cols = pd.MultiIndex.from_tuples(kept_cols, names=X.columns.names)\n",
    "    X_red = X.loc[:, kept_cols].copy()\n",
    "\n",
    "    # compact dtypes\n",
    "    X_red = cast_int8_multi(X_red, prefixes=(\"cross_\",), suffixes=(\"pattern\",))\n",
    "    other0 = [c for c in X_red.columns.get_level_values(0)\n",
    "              if not (str(c).startswith(\"cross_\") or str(c).endswith(\"pattern\"))]\n",
    "    if other0:\n",
    "        X_red.loc[:, (other0, slice(None))] = X_red.loc[:, (other0, slice(None))].astype('float32')\n",
    "\n",
    "    if verbose:\n",
    "        after_cnt = X_red.shape[1]\n",
    "        print(f\"[Feature reduction] columns: {before_cnt} → {after_cnt} ({100.0*after_cnt/before_cnt:.1f}% kept)\")\n",
    "    return X_red\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Construção por ticker\n",
    "# ============================\n",
    "feat_frames = []\n",
    "tgt_frames  = []\n",
    "\n",
    "print(\"Gerando features e targets por ticker...\")\n",
    "from collections import Counter\n",
    "\n",
    "feat_frames = []\n",
    "tgt_frames  = []\n",
    "skip_reasons = Counter()\n",
    "\n",
    "print(\"Gerando features e targets por ticker...\")\n",
    "for tk in tqdm(tickers):\n",
    "    try:\n",
    "        ohlcv = data.xs(tk, level=1, axis=1).copy()\n",
    "\n",
    "        # ✅ Require only what you truly need for features/targets\n",
    "        # Targets need High/Low; indicators need a close; SMAs may use Open but not required\n",
    "        have = set(ohlcv.columns.astype(str))\n",
    "        req = {'Close','High','Low'}   # do NOT require 'Adj Close' or 'Volume' here\n",
    "        if not req.issubset(have):\n",
    "            skip_reasons['missing_basic_ohlc'] += 1\n",
    "            continue\n",
    "\n",
    "        # Fallbacks to avoid skipping indices/FX/crypto\n",
    "        if 'Adj Close' not in have:\n",
    "            ohlcv['Adj Close'] = ohlcv['Close'].astype('float32')\n",
    "        if 'Open' not in have:\n",
    "            ohlcv['Open'] = ohlcv['Close'].astype('float32')\n",
    "        if 'Volume' not in have:\n",
    "            ohlcv['Volume'] = 0.0\n",
    "\n",
    "        # Build features\n",
    "        feats = indicators_for_ticker(ohlcv, shift_features=SHIFT_FEATURES)\n",
    "        pats  = patterns_for_ticker(ohlcv, shift_features=SHIFT_FEATURES)\n",
    "        if feats is None or feats.shape[1] == 0:\n",
    "            skip_reasons['empty_feats'] += 1\n",
    "            continue\n",
    "        X_tk  = pd.concat([feats, pats], axis=1)\n",
    "\n",
    "        # Fill & attach ticker level\n",
    "        X_tk = fill_100pct(X_tk, allow_bfill=ALLOW_BFILL_EXOGENOUS)\n",
    "        if X_tk.shape[1] == 0:\n",
    "            skip_reasons['empty_after_fill'] += 1\n",
    "            continue\n",
    "\n",
    "        X_tk.columns = pd.MultiIndex.from_product([X_tk.columns, [tk]])\n",
    "        feat_frames.append(X_tk)\n",
    "\n",
    "        # Targets (split up/down)\n",
    "        y_up20, y_dd5 = make_targets_up_down(\n",
    "            ohlcv, horizon=HORIZON, up_thr=0.20, dd_thr=-0.05, keep_order=False\n",
    "        )\n",
    "        y_up20.name = ('target_up20', tk)\n",
    "        y_dd5.name  = ('target_dd5',  tk)\n",
    "        tgt_frames.extend([y_up20, y_dd5])\n",
    "\n",
    "        # Best entry/sale (keep if you use downstream)\n",
    "        y_entry, y_sale = make_best_entry_sale(ohlcv, horizon=HORIZON)\n",
    "        y_entry.name = ('target_best_entry', tk)\n",
    "        y_sale.name  = ('target_best_sale',  tk)\n",
    "        tgt_frames.extend([y_entry, y_sale])\n",
    "\n",
    "    except Exception as e:\n",
    "        # don’t crash the whole build for one bad ticker\n",
    "        skip_reasons[f'exception:{type(e).__name__}'] += 1\n",
    "        # uncomment to inspect:\n",
    "        # print(f\"[{tk}] skipped due to {type(e).__name__}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Safety + diagnostics\n",
    "if len(feat_frames) == 0:\n",
    "    raise RuntimeError(f\"No features built for any ticker. Skip reasons: {dict(skip_reasons)}\")\n",
    "else:\n",
    "    print(\"Tickers processados:\", len(feat_frames), \"| Motivos de skip:\", dict(skip_reasons))\n",
    "\n",
    "# Concat globais\n",
    "X = pd.concat(feat_frames, axis=1).sort_index()\n",
    "y = pd.concat(tgt_frames,  axis=1).sort_index()\n",
    "\n",
    "# Cast consistente (atualizado p/ novos targets)\n",
    "X = cast_int8_multi(X, prefixes=(\"cross_\",), suffixes=(\"pattern\",))\n",
    "other0 = [c for c in X.columns.get_level_values(0)\n",
    "          if not (str(c).startswith(\"cross_\") or str(c).endswith(\"pattern\"))]\n",
    "if other0:\n",
    "    X.loc[:, (other0, slice(None))] = X.loc[:, (other0, slice(None))].astype('float32')\n",
    "\n",
    "for tname in ('target_up20','target_dd5'):\n",
    "    if tname in y.columns.get_level_values(0):\n",
    "        y.loc[:, (tname, slice(None))] = y.loc[:, (tname, slice(None))].astype('int8')\n",
    "for tname in ('target_best_entry','target_best_sale'):\n",
    "    if tname in y.columns.get_level_values(0):\n",
    "        y.loc[:, (tname, slice(None))] = y.loc[:, (tname, slice(None))].astype('float32')\n",
    "\n",
    "\n",
    "# Garantir 100% preenchido (inf/NaN) globalmente (features já estão ok; segurança adicional)\n",
    "X = fill_100pct(X, allow_bfill=ALLOW_BFILL_EXOGENOUS)\n",
    "# Alvos não devem ser imputados para evitar vazamento de informação no horizonte futuro\n",
    "y = y.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Verificações finais\n",
    "assert not X.isna().any().any(), \"Ainda há NaN em X após preenchimento!\"\n",
    "\n",
    "# ============================\n",
    "# Saída única combinada\n",
    "# ============================\n",
    "DATASET = pd.concat([X, y], axis=1).sort_index()\n",
    "# Segurança final (caso algum merge crie lacunas):\n",
    "DATASET = fill_100pct(DATASET, allow_bfill=ALLOW_BFILL_EXOGENOUS)\n",
    "\n",
    "# Sanidade: sem NaN\n",
    "assert not DATASET.isna().any().any(), \"Ainda há NaN no dataset final!\"\n",
    "\n",
    "# ============================\n",
    "# Feature reduction (per ticker, MI vs targets)\n",
    "# ============================\n",
    "# y_up and y_dd views (ensure they exist)\n",
    "y_up = y.loc[:, y.columns.get_level_values(0) == 'target_up20']\n",
    "y_dd = y.loc[:, y.columns.get_level_values(0) == 'target_dd5']\n",
    "\n",
    "X_reduced = reduce_features_automatic(\n",
    "    X, y_up, y_dd,\n",
    "    top_fraction=0.85,   # pega ~85% por MI antes de deduplicar\n",
    "    min_keep=96,         # garanta pelo menos ~100 por ticker (se existirem)\n",
    "    var_thr=None,        # não remova por variância agora\n",
    "    corr_thr=0.9995,     # só remove quase idênticos\n",
    "    always_keep_prefixes=(\"pct_change\",\"SMA_\",\"tri_\",\"sr_\"),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Output (full and reduced)\n",
    "# ============================\n",
    "DATASET_FULL = pd.concat([X, y], axis=1).sort_index()\n",
    "DATASET_FULL = fill_100pct(DATASET_FULL, allow_bfill=ALLOW_BFILL_EXOGENOUS)\n",
    "assert not DATASET_FULL.isna().any().any(), \"NaN in full dataset!\"\n",
    "\n",
    "DATASET_REDUCED = pd.concat([X_reduced, y], axis=1).sort_index()\n",
    "DATASET_REDUCED = fill_100pct(DATASET_REDUCED, allow_bfill=ALLOW_BFILL_EXOGENOUS)\n",
    "assert not DATASET_REDUCED.isna().any().any(), \"NaN in reduced dataset!\"\n",
    "\n",
    "if SAVE_PARQUET:\n",
    "    DATASET_FULL.to_parquet(OUTPUT_PATH, compression=\"snappy\")\n",
    "    OUT_REDUCED = OUTPUT_PATH.replace(\".parquet\", \"_reduced.parquet\")\n",
    "    DATASET_REDUCED.to_parquet(OUT_REDUCED, compression=\"snappy\")\n",
    "    print(f\"Saved FULL:     {OUTPUT_PATH}  shape={DATASET_FULL.shape}\")\n",
    "    print(f\"Saved REDUCED:  {OUT_REDUCED}  shape={DATASET_REDUCED.shape}\")\n",
    "else:\n",
    "    print(\"FULL:\", DATASET_FULL.shape, \" | REDUCED:\", DATASET_REDUCED.shape)\n",
    "\n",
    "print(\"X full:\", X.shape)\n",
    "print(\"y:\", y.shape)\n",
    "print(\"X_reduced:\", X_reduced.shape)\n",
    "\n",
    "# Per-ticker kept counts\n",
    "kept_counts = pd.Series(dict(\n",
    "    (tk, X_reduced.xs(tk, level=1, axis=1).shape[1])\n",
    "    for tk in np.unique(X_reduced.columns.get_level_values(1))\n",
    ")).sort_values(ascending=True)\n",
    "print(\"Min/median/max kept per ticker:\", kept_counts.min(), kept_counts.median(), kept_counts.max())\n",
    "\n",
    "processed = np.unique(X.columns.get_level_values(1))\n",
    "print(\"Processed tickers:\", len(processed))\n",
    "missing = sorted(set(tickers) - set(processed))\n",
    "print(\"Missing tickers:\", len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-MlRcKH_z8GQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762078614753,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Gabriel Juliao",
      "userId": "00918031370409536021"
     }
    },
    "outputId": "e3b87e10-88a9-4c48-f33b-abeab385ebaa"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           trend_visual_ichimoku_b volatility_dch trend_ichimoku_b  \\\n",
       "                          ABEV3.SA       ABEV3.SA         ABEV3.SA   \n",
       "Date                                                                 \n",
       "2025-10-29               12.315001          12.45        12.280001   \n",
       "2025-10-30               12.315001          12.66        12.280001   \n",
       "2025-10-31               12.315001          12.92        12.300000   \n",
       "2025-11-01               12.315001          12.92        12.300000   \n",
       "2025-11-02               12.295000          12.92        12.300000   \n",
       "\n",
       "           volatility_dcl    volume_obv   volume_nvi trend_ichimoku_base  \\\n",
       "                 ABEV3.SA      ABEV3.SA     ABEV3.SA            ABEV3.SA   \n",
       "Date                                                                       \n",
       "2025-10-29          11.74  3.082593e+10  2198.156982           12.065001   \n",
       "2025-10-30          11.74  3.091755e+10  2198.156982           12.170000   \n",
       "2025-10-31          11.74  3.094966e+10  2219.108398           12.300000   \n",
       "2025-11-01          11.75  3.098177e+10  2219.108398           12.300000   \n",
       "2025-11-02          11.75  3.101388e+10  2219.108398           12.300000   \n",
       "\n",
       "           trend_resistance2 mtb_close_roll_min chan_high_roll_max  ...  \\\n",
       "                    ABEV3.SA           ABEV3.SA           ABEV3.SA  ...   \n",
       "Date                                                                ...   \n",
       "2025-10-29             11.91              12.03              12.39  ...   \n",
       "2025-10-30             12.03              12.03              12.66  ...   \n",
       "2025-10-31             12.03              12.03              12.92  ...   \n",
       "2025-11-01             12.03              12.59              12.92  ...   \n",
       "2025-11-02             12.03              12.71              12.92  ...   \n",
       "\n",
       "           target_best_entry target_best_sale target_up20 target_dd5  \\\n",
       "                       ^JKSE            ^JKSE       ^N225      ^N225   \n",
       "Date                                                                   \n",
       "2025-10-29       8042.629883      8231.882812           0          0   \n",
       "2025-10-30       8144.077148      8231.882812           0          0   \n",
       "2025-10-31       8144.077148      8215.545898           0          0   \n",
       "2025-11-01       8144.077148      8215.545898           0          0   \n",
       "2025-11-02       8144.077148      8215.545898           0          0   \n",
       "\n",
       "           target_best_entry target_best_sale target_up20 target_dd5  \\\n",
       "                       ^N225            ^N225        ^SSE       ^SSE   \n",
       "Date                                                                   \n",
       "2025-10-29      50365.621094     52411.339844           1          1   \n",
       "2025-10-30      50972.558594     52411.339844           1          1   \n",
       "2025-10-31      51613.031250     52411.339844           1          1   \n",
       "2025-11-01      51613.031250     52411.339844           1          1   \n",
       "2025-11-02      51613.031250     52411.339844           0          0   \n",
       "\n",
       "           target_best_entry target_best_sale  \n",
       "                        ^SSE             ^SSE  \n",
       "Date                                           \n",
       "2025-10-29        100.029999       149.509995  \n",
       "2025-10-30        100.029999       149.509995  \n",
       "2025-10-31        100.029999       149.509995  \n",
       "2025-11-01        100.029999       149.509995  \n",
       "2025-11-02        100.029999       149.509995  \n",
       "\n",
       "[5 rows x 15023 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-6bf4cdca-2e26-4dc5-b964-b54202af7627\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>trend_visual_ichimoku_b</th>\n",
       "      <th>volatility_dch</th>\n",
       "      <th>trend_ichimoku_b</th>\n",
       "      <th>volatility_dcl</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>volume_nvi</th>\n",
       "      <th>trend_ichimoku_base</th>\n",
       "      <th>trend_resistance2</th>\n",
       "      <th>mtb_close_roll_min</th>\n",
       "      <th>chan_high_roll_max</th>\n",
       "      <th>...</th>\n",
       "      <th>target_best_entry</th>\n",
       "      <th>target_best_sale</th>\n",
       "      <th>target_up20</th>\n",
       "      <th>target_dd5</th>\n",
       "      <th>target_best_entry</th>\n",
       "      <th>target_best_sale</th>\n",
       "      <th>target_up20</th>\n",
       "      <th>target_dd5</th>\n",
       "      <th>target_best_entry</th>\n",
       "      <th>target_best_sale</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>...</th>\n",
       "      <th>^JKSE</th>\n",
       "      <th>^JKSE</th>\n",
       "      <th>^N225</th>\n",
       "      <th>^N225</th>\n",
       "      <th>^N225</th>\n",
       "      <th>^N225</th>\n",
       "      <th>^SSE</th>\n",
       "      <th>^SSE</th>\n",
       "      <th>^SSE</th>\n",
       "      <th>^SSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-10-29</th>\n",
       "      <td>12.315001</td>\n",
       "      <td>12.45</td>\n",
       "      <td>12.280001</td>\n",
       "      <td>11.74</td>\n",
       "      <td>3.082593e+10</td>\n",
       "      <td>2198.156982</td>\n",
       "      <td>12.065001</td>\n",
       "      <td>11.91</td>\n",
       "      <td>12.03</td>\n",
       "      <td>12.39</td>\n",
       "      <td>...</td>\n",
       "      <td>8042.629883</td>\n",
       "      <td>8231.882812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50365.621094</td>\n",
       "      <td>52411.339844</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.029999</td>\n",
       "      <td>149.509995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-30</th>\n",
       "      <td>12.315001</td>\n",
       "      <td>12.66</td>\n",
       "      <td>12.280001</td>\n",
       "      <td>11.74</td>\n",
       "      <td>3.091755e+10</td>\n",
       "      <td>2198.156982</td>\n",
       "      <td>12.170000</td>\n",
       "      <td>12.03</td>\n",
       "      <td>12.03</td>\n",
       "      <td>12.66</td>\n",
       "      <td>...</td>\n",
       "      <td>8144.077148</td>\n",
       "      <td>8231.882812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50972.558594</td>\n",
       "      <td>52411.339844</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.029999</td>\n",
       "      <td>149.509995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-31</th>\n",
       "      <td>12.315001</td>\n",
       "      <td>12.92</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>11.74</td>\n",
       "      <td>3.094966e+10</td>\n",
       "      <td>2219.108398</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>12.03</td>\n",
       "      <td>12.03</td>\n",
       "      <td>12.92</td>\n",
       "      <td>...</td>\n",
       "      <td>8144.077148</td>\n",
       "      <td>8215.545898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51613.031250</td>\n",
       "      <td>52411.339844</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.029999</td>\n",
       "      <td>149.509995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-01</th>\n",
       "      <td>12.315001</td>\n",
       "      <td>12.92</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>11.75</td>\n",
       "      <td>3.098177e+10</td>\n",
       "      <td>2219.108398</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>12.03</td>\n",
       "      <td>12.59</td>\n",
       "      <td>12.92</td>\n",
       "      <td>...</td>\n",
       "      <td>8144.077148</td>\n",
       "      <td>8215.545898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51613.031250</td>\n",
       "      <td>52411.339844</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.029999</td>\n",
       "      <td>149.509995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-02</th>\n",
       "      <td>12.295000</td>\n",
       "      <td>12.92</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>11.75</td>\n",
       "      <td>3.101388e+10</td>\n",
       "      <td>2219.108398</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>12.03</td>\n",
       "      <td>12.71</td>\n",
       "      <td>12.92</td>\n",
       "      <td>...</td>\n",
       "      <td>8144.077148</td>\n",
       "      <td>8215.545898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51613.031250</td>\n",
       "      <td>52411.339844</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.029999</td>\n",
       "      <td>149.509995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15023 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6bf4cdca-2e26-4dc5-b964-b54202af7627')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6bf4cdca-2e26-4dc5-b964-b54202af7627 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6bf4cdca-2e26-4dc5-b964-b54202af7627');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-f365e232-06c7-4403-9127-a5ef347e5f09\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f365e232-06c7-4403-9127-a5ef347e5f09')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-f365e232-06c7-4403-9127-a5ef347e5f09 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe"
      }
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "DATASET_REDUCED.tail()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "16Uuck_eS8s48EnvQSl6QRwXlaog7j4iJ",
     "timestamp": 1761488811461
    }
   ],
   "authorship_tag": "ABX9TyNLoL2vQYlmGZMn0TlkSTdr"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}